{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cca9fd83-a371-468d-965b-96ad8c972d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Course & Resume Keyword Extractor...\n",
      "  Loading spaCy model...\n",
      "âœ“ Extractor initialized\n",
      "\n",
      "Created sample CSV: sample_courses.csv\n",
      "\n",
      "================================================================================\n",
      "PROCESSING COURSES + RESUME CSV\n",
      "================================================================================\n",
      "\n",
      "Reading CSV: sample_courses.csv\n",
      "  Found 4 courses\n",
      "\n",
      "Processing course descriptions...\n",
      "  [1/4] STAT 215 - Statistical Inference\n",
      "  [2/4] CS 101 - Introduction to Programming\n",
      "  [3/4] ECON 301 - Applied Econometrics\n",
      "  [4/4] EDUC 200 - Education Policy\n",
      "\n",
      "âœ“ Processed 4 courses\n",
      "\n",
      "Processing resume...\n",
      "  âœ“ Extracted 50 resume keywords\n",
      "\n",
      "Creating unified keyword master list...\n",
      "  âœ“ Unified list contains 100 unique keywords\n",
      "\n",
      "================================================================================\n",
      "PROCESSING COMPLETE\n",
      "================================================================================\n",
      "\n",
      "Saved results to: extracted_keywords.json\n",
      "\n",
      "================================================================================\n",
      "RESULTS SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Statistics:\n",
      "  â€¢ total_courses: 4\n",
      "  â€¢ total_course_keywords: 200\n",
      "  â€¢ total_resume_keywords: 50\n",
      "  â€¢ total_unique_keywords: 100\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Course Keywords (sample):\n",
      "\n",
      "STAT 215 - Statistical Inference:\n",
      "  Keywords: statistical inference, regression, analysis of variance, statistical inference hypothesis, inference hypothesis test, hypothesis test confidence, test confidence interval, confidence interval regression, interval regression analysis, regression analysis apply...\n",
      "\n",
      "CS 101 - Introduction to Programming:\n",
      "  Keywords: python, algorithm, javascript, object oriented programming, application programming interface, hypertext markup language, cascading style sheets, fundamental programming concept, programming concept python, concept python topic...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Resume Keywords (first 20):\n",
      "  machine learning, python, visualization, tableau, statistics, sql, java, excel, git, research, communication, teamwork, leadership, structured query language, amazon web services, john doe education, doe education bachelor, education bachelor science, bachelor science statistic, science statistic minor\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Unified Master List (first 30):\n",
      "  statistical inference, time series, causal inference, machine learning, regression, python, algorithm, javascript, visualization, tableau, statistics, sql, java, excel, git, research, communication, teamwork, leadership, analysis of variance, statistical inference hypothesis, inference hypothesis test, hypothesis test confidence, test confidence interval, confidence interval regression, interval regression analysis, regression analysis apply, analysis apply statistical, apply statistical method, statistical method real\n",
      "\n",
      "================================================================================\n",
      "\n",
      "==========================================================================================\n",
      "RUNNING JOB DESCRIPTION PIPELINE DEMO\n",
      "==========================================================================================\n",
      "\n",
      "Initializing Course & Resume Keyword Extractor...\n",
      "  Loading spaCy model...\n",
      "âœ“ Extractor initialized\n",
      "\n",
      "\n",
      "ðŸ”¹ Loading SentenceTransformer model...\n",
      "   âœ“ Embedding model loaded\n",
      "\n",
      "==========================================================================================\n",
      "PROCESSING JOB LISTINGS + MATCHING\n",
      "==========================================================================================\n",
      "\n",
      "  â–¶ [1/4] Extracting â†’ Data Scientist Intern\n",
      "  â–¶ [2/4] Extracting â†’ Product Strategy Analyst\n",
      "  â–¶ [3/4] Extracting â†’ Machine Learning Engineer\n",
      "  â–¶ [4/4] Extracting â†’ Bioinformatics Research Assistant\n",
      "\n",
      "âœ“ Finished extracting job keywords\n",
      "\n",
      "Embedding USER master keyword list...\n",
      "Computing semantic similarity...\n",
      "\n",
      "ðŸ”¥ TOP 3 MATCHES:\n",
      "\n",
      "â­ Data Scientist Intern â†’ 0.7073\n",
      "â­ Product Strategy Analyst â†’ 0.6841\n",
      "â­ Machine Learning Engineer â†’ 0.6682\n",
      "\n",
      "âœ“ SEMANTIC MATCHING COMPLETE\n",
      "\n",
      "\n",
      "Saved â†’ job_keywords.json\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "# pip install spacy\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# COURSE SYLLABI + RESUME KEYWORD EXTRACTION PIPELINE\n",
    "# For Canvas-Career Bridge Matching System\n",
    "# ============================================================================\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "from typing import List, Dict, Set, Tuple\n",
    "import spacy\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Optional: Install if needed\n",
    "# !pip install spacy yake-keyword pandas\n",
    "# !python -m spacy download en_core_web_sm\n",
    "\n",
    "\n",
    "class CourseResumeKeywordExtractor:\n",
    "    \"\"\"\n",
    "    Extract and normalize keywords from course descriptions and resumes.\n",
    "    Uses spaCy for NLP, with n-gram generation and abbreviation expansion.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize NLP models and lookup dictionaries.\"\"\"\n",
    "        \n",
    "        print(\"Initializing Course & Resume Keyword Extractor...\")\n",
    "        \n",
    "        # Load spaCy model for NLP\n",
    "        print(\"  Loading spaCy model...\")\n",
    "        try:\n",
    "            self.nlp = spacy.load('en_core_web_sm')\n",
    "        except:\n",
    "            print(\"  Downloading spaCy model...\")\n",
    "            import os\n",
    "            os.system('python -m spacy download en_core_web_sm')\n",
    "            self.nlp = spacy.load('en_core_web_sm')\n",
    "        \n",
    "        # ====================================================================\n",
    "        # STOPWORDS\n",
    "        # ====================================================================\n",
    "        self.stopwords = {\n",
    "            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',\n",
    "            'of', 'with', 'by', 'from', 'as', 'is', 'was', 'are', 'been', 'be',\n",
    "            'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'should',\n",
    "            'could', 'may', 'might', 'must', 'can', 'this', 'that', 'these', 'those',\n",
    "            'we', 'you', 'they', 'them', 'their', 'our', 'your', 'my', 'me', 'i',\n",
    "            'he', 'she', 'it', 'who', 'what', 'where', 'when', 'why', 'how',\n",
    "            'all', 'each', 'every', 'both', 'few', 'more', 'most', 'other', 'some',\n",
    "            'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too',\n",
    "            'very', 'just', 'course', 'student', 'students', 'class', 'semester',\n",
    "            'week', 'weeks', 'include', 'includes', 'including', 'also', 'well',\n",
    "            'use', 'using', 'used', 'learn', 'learning', 'introduce', 'introduction'\n",
    "        }\n",
    "        \n",
    "        # ====================================================================\n",
    "        # ABBREVIATION EXPANSION DICTIONARY\n",
    "        # ====================================================================\n",
    "        self.abbreviation_map = {\n",
    "            # Machine Learning & AI\n",
    "            'ml': 'machine learning',\n",
    "            'ai': 'artificial intelligence',\n",
    "            'nlp': 'natural language processing',\n",
    "            'cv': 'computer vision',\n",
    "            'dl': 'deep learning',\n",
    "            'nn': 'neural networks',\n",
    "            'cnn': 'convolutional neural networks',\n",
    "            'rnn': 'recurrent neural networks',\n",
    "            'gan': 'generative adversarial networks',\n",
    "            \n",
    "            # Programming & Development\n",
    "            'oop': 'object oriented programming',\n",
    "            'api': 'application programming interface',\n",
    "            'rest': 'representational state transfer',\n",
    "            'crud': 'create read update delete',\n",
    "            'mvc': 'model view controller',\n",
    "            'ui': 'user interface',\n",
    "            'ux': 'user experience',\n",
    "            'sdk': 'software development kit',\n",
    "            'ide': 'integrated development environment',\n",
    "            \n",
    "            # Data & Databases\n",
    "            'sql': 'structured query language',\n",
    "            'nosql': 'non-relational database',\n",
    "            'rdbms': 'relational database management system',\n",
    "            'etl': 'extract transform load',\n",
    "            'olap': 'online analytical processing',\n",
    "            'oltp': 'online transaction processing',\n",
    "            'bi': 'business intelligence',\n",
    "            'eda': 'exploratory data analysis',\n",
    "            \n",
    "            # Statistics & Analysis\n",
    "            'anova': 'analysis of variance',\n",
    "            'regression': 'regression analysis',\n",
    "            'pca': 'principal component analysis',\n",
    "            'svm': 'support vector machine',\n",
    "            'knn': 'k nearest neighbors',\n",
    "            'rf': 'random forest',\n",
    "            \n",
    "            # Cloud & DevOps\n",
    "            'aws': 'amazon web services',\n",
    "            'gcp': 'google cloud platform',\n",
    "            'cicd': 'continuous integration continuous deployment',\n",
    "            'ci/cd': 'continuous integration continuous deployment',\n",
    "            'vm': 'virtual machine',\n",
    "            \n",
    "            # Business & Management\n",
    "            'crm': 'customer relationship management',\n",
    "            'erp': 'enterprise resource planning',\n",
    "            'roi': 'return on investment',\n",
    "            'kpi': 'key performance indicator',\n",
    "            'b2b': 'business to business',\n",
    "            'b2c': 'business to consumer',\n",
    "            'saas': 'software as a service',\n",
    "            'paas': 'platform as a service',\n",
    "            'iaas': 'infrastructure as a service',\n",
    "            \n",
    "            # Academic & Research\n",
    "            'apa': 'american psychological association',\n",
    "            'mla': 'modern language association',\n",
    "            'gpa': 'grade point average',\n",
    "            'stem': 'science technology engineering mathematics',\n",
    "            \n",
    "            # Other\n",
    "            'html': 'hypertext markup language',\n",
    "            'css': 'cascading style sheets',\n",
    "            'xml': 'extensible markup language',\n",
    "            'json': 'javascript object notation',\n",
    "            'http': 'hypertext transfer protocol',\n",
    "            'https': 'hypertext transfer protocol secure',\n",
    "            'url': 'uniform resource locator',\n",
    "            'gui': 'graphical user interface',\n",
    "            'cli': 'command line interface',\n",
    "            'os': 'operating system',\n",
    "            'io': 'input output',\n",
    "            'ar': 'augmented reality',\n",
    "            'vr': 'virtual reality',\n",
    "            'iot': 'internet of things',\n",
    "            'gis': 'geographic information system'\n",
    "        }\n",
    "        \n",
    "        # ====================================================================\n",
    "        # SYNONYM NORMALIZATION\n",
    "        # ====================================================================\n",
    "        self.synonym_map = {\n",
    "            # Programming synonyms\n",
    "            'coding': 'programming',\n",
    "            'software development': 'programming',\n",
    "            'software engineering': 'programming',\n",
    "            'scripting': 'programming',\n",
    "            \n",
    "            # Data synonyms\n",
    "            'data science': 'data analysis',\n",
    "            'analytics': 'data analysis',\n",
    "            'data analytics': 'data analysis',\n",
    "            'statistical analysis': 'statistics',\n",
    "            'statistical methods': 'statistics',\n",
    "            'quantitative analysis': 'statistics',\n",
    "            'quantitative methods': 'statistics',\n",
    "            \n",
    "            # Database synonyms\n",
    "            'database management': 'database',\n",
    "            'data storage': 'database',\n",
    "            'data warehouse': 'database',\n",
    "            \n",
    "            # Modeling synonyms\n",
    "            'predictive modeling': 'modeling',\n",
    "            'statistical modeling': 'modeling',\n",
    "            'mathematical modeling': 'modeling',\n",
    "            \n",
    "            # Visualization synonyms\n",
    "            'data visualization': 'visualization',\n",
    "            'visual analytics': 'visualization',\n",
    "            'graphical analysis': 'visualization',\n",
    "            \n",
    "            # Research synonyms\n",
    "            'research methods': 'research',\n",
    "            'research design': 'research',\n",
    "            'empirical research': 'research',\n",
    "            \n",
    "            # Analysis synonyms\n",
    "            'econometric analysis': 'econometrics',\n",
    "            'regression modeling': 'regression',\n",
    "            'time series analysis': 'time series',\n",
    "            \n",
    "            # Communication synonyms\n",
    "            'technical writing': 'writing',\n",
    "            'business writing': 'writing',\n",
    "            'oral presentation': 'presentation',\n",
    "            'public speaking': 'presentation'\n",
    "        }\n",
    "        \n",
    "        # ====================================================================\n",
    "        # IMPORTANT SKILLS/CONCEPTS TO PRIORITIZE\n",
    "        # ====================================================================\n",
    "        self.important_terms = {\n",
    "            # Technical skills\n",
    "            'python', 'r', 'java', 'javascript', 'sql', 'c++', 'matlab',\n",
    "            'tableau', 'excel', 'power bi', 'git', 'docker', 'kubernetes',\n",
    "            \n",
    "            # Methodologies\n",
    "            'machine learning', 'deep learning', 'data analysis', 'statistics',\n",
    "            'regression', 'hypothesis testing', 'statistical inference',\n",
    "            'econometrics', 'time series', 'panel data', 'causal inference',\n",
    "            \n",
    "            # Domain concepts\n",
    "            'optimization', 'simulation', 'modeling', 'forecasting',\n",
    "            'algorithm', 'data structure', 'database', 'visualization',\n",
    "            'research', 'experimentation', 'survey design', 'sampling',\n",
    "            \n",
    "            # Soft skills\n",
    "            'communication', 'teamwork', 'leadership', 'problem solving',\n",
    "            'critical thinking', 'project management', 'presentation',\n",
    "            \n",
    "            # Business concepts\n",
    "            'supply chain', 'operations', 'finance', 'marketing', 'strategy',\n",
    "            'policy analysis', 'economic analysis', 'business analysis'\n",
    "        }\n",
    "        \n",
    "        print(\"âœ“ Extractor initialized\\n\")\n",
    "    \n",
    "    \n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Clean and normalize text.\n",
    "        \n",
    "        Steps:\n",
    "        1. Convert to lowercase\n",
    "        2. Remove HTML tags\n",
    "        3. Remove special characters but keep spaces and hyphens\n",
    "        4. Remove extra whitespace\n",
    "        \"\"\"\n",
    "        if not text or not isinstance(text, str):\n",
    "            return \"\"\n",
    "        \n",
    "        # Lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove HTML tags\n",
    "        text = re.sub(r'<[^>]+>', '', text)\n",
    "        \n",
    "        # Remove URLs\n",
    "        text = re.sub(r'http\\S+|www.\\S+', '', text)\n",
    "        \n",
    "        # Remove email addresses\n",
    "        text = re.sub(r'\\S+@\\S+', '', text)\n",
    "        \n",
    "        # Remove special characters but keep spaces, hyphens, and forward slashes\n",
    "        text = re.sub(r'[^\\w\\s/-]', ' ', text)\n",
    "        \n",
    "        # Remove standalone numbers (but keep numbers within words like \"cs101\")\n",
    "        text = re.sub(r'\\b\\d+\\b', '', text)\n",
    "        \n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        \n",
    "        return text.strip()\n",
    "    \n",
    "    \n",
    "    def tokenize_and_lemmatize(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Tokenize text and lemmatize using spaCy.\n",
    "        \n",
    "        Returns:\n",
    "        - List of lemmatized tokens (excluding stopwords and short words)\n",
    "        \"\"\"\n",
    "        doc = self.nlp(text)\n",
    "        \n",
    "        tokens = []\n",
    "        for token in doc:\n",
    "            # Skip if it's a stopword, punctuation, or very short\n",
    "            if (token.text.lower() in self.stopwords or \n",
    "                token.is_punct or \n",
    "                token.is_space or \n",
    "                len(token.text) < 2):\n",
    "                continue\n",
    "            \n",
    "            # Use lemma (base form of word)\n",
    "            lemma = token.lemma_.lower()\n",
    "            \n",
    "            # Skip if lemmatized form is a stopword\n",
    "            if lemma not in self.stopwords:\n",
    "                tokens.append(lemma)\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    \n",
    "    def generate_ngrams(self, tokens: List[str], max_n: int = 3) -> List[str]:\n",
    "        \"\"\"\n",
    "        Generate n-grams (1-grams, 2-grams, 3-grams).\n",
    "        \n",
    "        Prioritizes academically meaningful phrases.\n",
    "        \"\"\"\n",
    "        ngrams = []\n",
    "        \n",
    "        # Add unigrams\n",
    "        ngrams.extend(tokens)\n",
    "        \n",
    "        # Add bigrams\n",
    "        for i in range(len(tokens) - 1):\n",
    "            bigram = f\"{tokens[i]} {tokens[i+1]}\"\n",
    "            ngrams.append(bigram)\n",
    "        \n",
    "        # Add trigrams\n",
    "        for i in range(len(tokens) - 2):\n",
    "            trigram = f\"{tokens[i]} {tokens[i+1]} {tokens[i+2]}\"\n",
    "            ngrams.append(trigram)\n",
    "        \n",
    "        return ngrams\n",
    "    \n",
    "    \n",
    "    def expand_abbreviations(self, ngrams: List[str]) -> Tuple[List[str], List[str]]:\n",
    "        \"\"\"\n",
    "        Expand abbreviations found in n-grams.\n",
    "        \n",
    "        Returns:\n",
    "        - expanded_ngrams: n-grams with abbreviations expanded\n",
    "        - expansions_found: list of (abbrev, expansion) pairs found\n",
    "        \"\"\"\n",
    "        expanded_ngrams = []\n",
    "        expansions_found = []\n",
    "        \n",
    "        for ngram in ngrams:\n",
    "            if ngram in self.abbreviation_map:\n",
    "                # Found an abbreviation\n",
    "                expansion = self.abbreviation_map[ngram]\n",
    "                expanded_ngrams.append(expansion)\n",
    "                expansions_found.append(f\"{ngram} â†’ {expansion}\")\n",
    "                # Also keep the original abbreviation\n",
    "                expanded_ngrams.append(ngram)\n",
    "            else:\n",
    "                expanded_ngrams.append(ngram)\n",
    "        \n",
    "        return expanded_ngrams, expansions_found\n",
    "    \n",
    "    \n",
    "    def normalize_synonyms(self, ngrams: List[str]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Normalize synonyms to canonical forms.\n",
    "        \"\"\"\n",
    "        normalized = []\n",
    "        \n",
    "        for ngram in ngrams:\n",
    "            if ngram in self.synonym_map:\n",
    "                canonical = self.synonym_map[ngram]\n",
    "                normalized.append(canonical)\n",
    "            else:\n",
    "                normalized.append(ngram)\n",
    "        \n",
    "        return normalized\n",
    "    \n",
    "    \n",
    "    def extract_final_keywords(self, ngrams: List[str], top_n: int = 50) -> List[str]:\n",
    "        \"\"\"\n",
    "        Extract final keywords by:\n",
    "        1. Removing duplicates\n",
    "        2. Prioritizing important terms\n",
    "        3. Filtering by frequency\n",
    "        4. Preferring longer phrases\n",
    "        \"\"\"\n",
    "        # Count frequencies\n",
    "        ngram_counts = Counter(ngrams)\n",
    "        \n",
    "        # Separate into important and other\n",
    "        important_keywords = []\n",
    "        other_keywords = []\n",
    "        \n",
    "        for ngram, count in ngram_counts.items():\n",
    "            if ngram in self.important_terms:\n",
    "                important_keywords.append((ngram, count, len(ngram.split())))\n",
    "            else:\n",
    "                other_keywords.append((ngram, count, len(ngram.split())))\n",
    "        \n",
    "        # Sort important keywords by: length (longer = better), then frequency\n",
    "        important_keywords.sort(key=lambda x: (x[2], x[1]), reverse=True)\n",
    "        \n",
    "        # Sort other keywords similarly\n",
    "        other_keywords.sort(key=lambda x: (x[2], x[1]), reverse=True)\n",
    "        \n",
    "        # Combine: prioritize important terms\n",
    "        final_keywords = (\n",
    "            [kw[0] for kw in important_keywords] + \n",
    "            [kw[0] for kw in other_keywords]\n",
    "        )\n",
    "        \n",
    "        # Remove duplicates while preserving order\n",
    "        seen = set()\n",
    "        unique_keywords = []\n",
    "        for kw in final_keywords:\n",
    "            if kw not in seen:\n",
    "                seen.add(kw)\n",
    "                unique_keywords.append(kw)\n",
    "        \n",
    "        return unique_keywords[:top_n]\n",
    "    \n",
    "    \n",
    "    def process_text(self, text: str, text_type: str = \"course\") -> Dict:\n",
    "        \"\"\"\n",
    "        Complete pipeline to process text and extract keywords.\n",
    "        \n",
    "        Args:\n",
    "            text: Input text (course description or resume)\n",
    "            text_type: \"course\" or \"resume\" (for logging)\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with all intermediate and final results\n",
    "        \"\"\"\n",
    "        # Step 1: Clean text\n",
    "        cleaned_text = self.clean_text(text)\n",
    "        \n",
    "        # Step 2: Tokenize and lemmatize\n",
    "        tokens = self.tokenize_and_lemmatize(cleaned_text)\n",
    "        \n",
    "        # Step 3: Generate n-grams\n",
    "        ngrams = self.generate_ngrams(tokens, max_n=3)\n",
    "        \n",
    "        # Step 4: Expand abbreviations\n",
    "        expanded_ngrams, expansions = self.expand_abbreviations(ngrams)\n",
    "        \n",
    "        # Step 5: Normalize synonyms\n",
    "        normalized_ngrams = self.normalize_synonyms(expanded_ngrams)\n",
    "        \n",
    "        # Step 6: Extract final keywords\n",
    "        final_keywords = self.extract_final_keywords(normalized_ngrams, top_n=50)\n",
    "        \n",
    "        return {\n",
    "            \"cleaned_text\": cleaned_text,\n",
    "            \"tokens\": tokens,\n",
    "            \"ngrams\": ngrams[:20],  # Sample for debugging\n",
    "            \"expanded_abbreviations\": expansions,\n",
    "            \"final_keywords\": final_keywords\n",
    "        }\n",
    "    \n",
    "    \n",
    "    def process_courses_csv(self, csv_path: str, \n",
    "                           course_name_col: str = 'course_name',\n",
    "                           course_desc_col: str = 'course_description',\n",
    "                           resume_col: str = 'resume') -> Dict:\n",
    "        \"\"\"\n",
    "        Process CSV file containing courses and resume.\n",
    "        \n",
    "        Args:\n",
    "            csv_path: Path to CSV file\n",
    "            course_name_col: Column name for course names\n",
    "            course_desc_col: Column name for course descriptions\n",
    "            resume_col: Column name for resume text\n",
    "            \n",
    "        Returns:\n",
    "            Complete structured output with all keywords\n",
    "        \"\"\"\n",
    "        print(\"=\"*80)\n",
    "        print(\"PROCESSING COURSES + RESUME CSV\")\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "        \n",
    "        # Read CSV\n",
    "        print(f\"Reading CSV: {csv_path}\")\n",
    "        df = pd.read_csv(csv_path)\n",
    "        print(f\"  Found {len(df)} courses\\n\")\n",
    "        \n",
    "        # ====================================================================\n",
    "        # PROCESS COURSES\n",
    "        # ====================================================================\n",
    "        print(\"Processing course descriptions...\")\n",
    "        \n",
    "        courses_output = []\n",
    "        all_course_keywords = []\n",
    "        \n",
    "        for idx, row in df.iterrows():\n",
    "            course_name = row[course_name_col]\n",
    "            course_desc = row[course_desc_col]\n",
    "            \n",
    "            print(f\"  [{idx+1}/{len(df)}] {course_name}\")\n",
    "            \n",
    "            # Process this course description\n",
    "            result = self.process_text(course_desc, text_type=\"course\")\n",
    "            \n",
    "            # Store course keywords\n",
    "            courses_output.append({\n",
    "                \"course_name\": course_name,\n",
    "                \"keywords\": result[\"final_keywords\"]\n",
    "            })\n",
    "            \n",
    "            # Add to master list\n",
    "            all_course_keywords.extend(result[\"final_keywords\"])\n",
    "        \n",
    "        print(f\"\\nâœ“ Processed {len(courses_output)} courses\\n\")\n",
    "        \n",
    "        # ====================================================================\n",
    "        # PROCESS RESUME (only once since it's the same in all rows)\n",
    "        # ====================================================================\n",
    "        print(\"Processing resume...\")\n",
    "        \n",
    "        resume_text = df[resume_col].iloc[0]  # Get from first row\n",
    "        resume_result = self.process_text(resume_text, text_type=\"resume\")\n",
    "        resume_keywords = resume_result[\"final_keywords\"]\n",
    "        \n",
    "        print(f\"  âœ“ Extracted {len(resume_keywords)} resume keywords\\n\")\n",
    "        \n",
    "        # ====================================================================\n",
    "        # CREATE UNIFIED MASTER LIST\n",
    "        # ====================================================================\n",
    "        print(\"Creating unified keyword master list...\")\n",
    "        \n",
    "        # Combine all keywords\n",
    "        all_keywords_combined = all_course_keywords + resume_keywords\n",
    "        \n",
    "        # Remove duplicates while preserving importance\n",
    "        all_keywords_unique = self.extract_final_keywords(all_keywords_combined, top_n=100)\n",
    "        \n",
    "        print(f\"  âœ“ Unified list contains {len(all_keywords_unique)} unique keywords\\n\")\n",
    "        \n",
    "        # ====================================================================\n",
    "        # BUILD FINAL OUTPUT\n",
    "        # ====================================================================\n",
    "        output = {\n",
    "            \"courses\": courses_output,\n",
    "            \"resume_keywords\": resume_keywords,\n",
    "            \"all_keywords\": all_keywords_unique,\n",
    "            \"statistics\": {\n",
    "                \"total_courses\": len(courses_output),\n",
    "                \"total_course_keywords\": len(all_course_keywords),\n",
    "                \"total_resume_keywords\": len(resume_keywords),\n",
    "                \"total_unique_keywords\": len(all_keywords_unique)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        print(\"=\"*80)\n",
    "        print(\"PROCESSING COMPLETE\")\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# USAGE EXAMPLE\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Initialize extractor\n",
    "    extractor = CourseResumeKeywordExtractor()\n",
    "    \n",
    "    # ========================================================================\n",
    "    # CREATE SAMPLE CSV FOR DEMONSTRATION\n",
    "    # ========================================================================\n",
    "    \n",
    "    sample_data = pd.DataFrame({\n",
    "        'course_name': [\n",
    "            'STAT 215 - Statistical Inference',\n",
    "            'CS 101 - Introduction to Programming',\n",
    "            'ECON 301 - Applied Econometrics',\n",
    "            'EDUC 200 - Education Policy'\n",
    "        ],\n",
    "        'course_description': [\n",
    "            'Introduction to statistical inference including hypothesis testing, confidence intervals, and regression analysis. Students will learn to apply statistical methods to real-world data using R programming. Topics include ANOVA, linear regression, and experimental design.',\n",
    "            'Fundamental programming concepts using Python. Topics include data structures, algorithms, OOP principles, and API development. Students will build projects using HTML, CSS, and JavaScript.',\n",
    "            'Application of statistical and econometric methods to economic data. Focus on regression models, panel data analysis, time series, and causal inference techniques. Use of Stata and R for analysis.',\n",
    "            'Examination of contemporary education policy issues including equity, access, and educational outcomes. Analysis of policy interventions using data-driven approaches and program evaluation methods.'\n",
    "        ],\n",
    "        'resume': [\n",
    "            # Same resume in all rows\n",
    "            '''John Doe\n",
    "            Education: Bachelor of Science in Statistics, Minor in Computer Science\n",
    "            \n",
    "            Skills:\n",
    "            - Programming: Python, R, SQL, Java\n",
    "            - Data Analysis: Statistical Analysis, ML, Data Visualization, Tableau\n",
    "            - Tools: Excel, Git, AWS\n",
    "            \n",
    "            Experience:\n",
    "            Research Assistant | Dept of Education | 2024-Present\n",
    "            - Statistical analysis using R and Python\n",
    "            - Created visualizations using Tableau\n",
    "            - ML models for predictive analytics\n",
    "            \n",
    "            Data Science Club President | 2023-Present\n",
    "            - Led workshops on data analysis\n",
    "            - Organized hackathons\n",
    "            \n",
    "            Soft Skills: Communication, Teamwork, Leadership, Problem Solving'''\n",
    "        ] * 4  # Repeat same resume for all rows\n",
    "    })\n",
    "    \n",
    "    # Save to CSV\n",
    "    sample_data.to_csv('sample_courses.csv', index=False)\n",
    "    print(\"Created sample CSV: sample_courses.csv\\n\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # PROCESS THE CSV\n",
    "    # ========================================================================\n",
    "    \n",
    "    results = extractor.process_courses_csv(\n",
    "        'sample_courses.csv',\n",
    "        course_name_col='course_name',\n",
    "        course_desc_col='course_description',\n",
    "        resume_col='resume'\n",
    "    )\n",
    "    \n",
    "    # ========================================================================\n",
    "    # SAVE TO JSON\n",
    "    # ========================================================================\n",
    "    \n",
    "    with open('extracted_keywords.json', 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    print(\"Saved results to: extracted_keywords.json\\n\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # DISPLAY RESULTS\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"RESULTS SUMMARY\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    print(f\"Statistics:\")\n",
    "    for key, value in results['statistics'].items():\n",
    "        print(f\"  â€¢ {key}: {value}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*80 + \"\\n\")\n",
    "    \n",
    "    print(\"Course Keywords (sample):\")\n",
    "    for course in results['courses'][:2]:  # Show first 2\n",
    "        print(f\"\\n{course['course_name']}:\")\n",
    "        print(f\"  Keywords: {', '.join(course['keywords'][:10])}...\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*80 + \"\\n\")\n",
    "    \n",
    "    print(\"Resume Keywords (first 20):\")\n",
    "    print(f\"  {', '.join(results['resume_keywords'][:20])}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*80 + \"\\n\")\n",
    "    \n",
    "    print(\"Unified Master List (first 30):\")\n",
    "    print(f\"  {', '.join(results['all_keywords'][:30])}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# JOB LISTING SEMANTIC MATCHING PIPELINE\n",
    "# ============================================================================\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class JobKeywordExtractor(CourseResumeKeywordExtractor):\n",
    "    \"\"\"\n",
    "    Extracts normalized keywords for each job listing, embeds them using\n",
    "    SentenceTransformer, embeds the user's unified keyword list, performs\n",
    "    semantic matching using cosine similarity, and returns the TOP 3 JOBS.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        print(\"\\nðŸ”¹ Loading SentenceTransformer model...\")\n",
    "        self.embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "        print(\"   âœ“ Embedding model loaded\\n\")\n",
    "\n",
    "    def embed(self, text: str) -> np.ndarray:\n",
    "        \"\"\"Return L2-normalized embedding for a text string\"\"\"\n",
    "        return self.embedder.encode(text, normalize_embeddings=True)\n",
    "\n",
    "    \n",
    "\n",
    "    def process_jobs_csv(self,\n",
    "                         csv_path: str,\n",
    "                         job_name_col: str = \"job_name\",\n",
    "                         job_desc_col: str = \"job_description\",\n",
    "                         job_link_col: str = \"job_link\") -> dict:\n",
    "\n",
    "        print(\"=\"*90)\n",
    "        print(\"PROCESSING JOB LISTINGS + MATCHING\")\n",
    "        print(\"=\"*90 + \"\\n\")\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path, encoding=\"utf-8\")\n",
    "        except UnicodeDecodeError:\n",
    "            print(\"âš  UTF-8 failed â†’ trying latin-1\")\n",
    "            df = pd.read_csv(csv_path, encoding=\"latin-1\")\n",
    "            print(f\"Found {len(df)} job listings\\n\")\n",
    "\n",
    "        job_keyword_dict = {}\n",
    "\n",
    "        # ==========================================================\n",
    "        # 1ï¸âƒ£ Extract keywords per job\n",
    "        # ==========================================================\n",
    "        for i, row in df.iterrows():\n",
    "\n",
    "            job_name = str(row[job_name_col])\n",
    "            job_text = str(row[job_desc_col]) if pd.notna(row[job_desc_col]) else \"\"\n",
    "\n",
    "            print(f\"  â–¶ [{i+1}/{len(df)}] Extracting â†’ {job_name}\")\n",
    "\n",
    "            result = self.process_text(job_text, text_type=\"job\")\n",
    "            job_keyword_dict[job_name] = result[\"final_keywords\"]\n",
    "\n",
    "        print(\"\\nâœ“ Finished extracting job keywords\\n\")\n",
    "\n",
    "        # ==========================================================\n",
    "        # 2ï¸âƒ£ Embed USER unified keyword list\n",
    "        # ==========================================================\n",
    "        print(\"Embedding USER master keyword list...\")\n",
    "\n",
    "        # Combine master list into one text block\n",
    "        user_keywords = []\n",
    "        for job_list in job_keyword_dict.values():\n",
    "            user_keywords.extend(job_list)\n",
    "\n",
    "        user_keywords = list(set(user_keywords))     # dedupe\n",
    "        user_text = \" ; \".join(user_keywords)\n",
    "        user_vec = self.embed(user_text).reshape(1, -1)\n",
    "\n",
    "        # ==========================================================\n",
    "        # 3ï¸âƒ£ Embed each JOB & score similarity\n",
    "        # ==========================================================\n",
    "        print(\"Computing semantic similarity...\\n\")\n",
    "\n",
    "        scored_jobs = []\n",
    "\n",
    "        for job_name, kw_list in job_keyword_dict.items():\n",
    "\n",
    "            job_text = \" ; \".join(kw_list)\n",
    "            job_vec = self.embed(job_text).reshape(1, -1)\n",
    "\n",
    "            score = float(cosine_similarity(user_vec, job_vec)[0][0])\n",
    "\n",
    "            scored_jobs.append((job_name, score))\n",
    "\n",
    "        scored_jobs.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # ==========================================================\n",
    "        # 4ï¸âƒ£ RETURN ONLY TOP 3\n",
    "        # ==========================================================\n",
    "        top_matches = [\n",
    "            {\"job\": job, \"score\": float(f\"{score:.4f}\")}\n",
    "            for job, score in scored_jobs[:3]\n",
    "        ]\n",
    "\n",
    "        print(\"ðŸ”¥ TOP 3 MATCHES:\\n\")\n",
    "        for m in top_matches:\n",
    "            print(f\"â­ {m['job']} â†’ {m['score']}\")\n",
    "\n",
    "        print(\"\\nâœ“ SEMANTIC MATCHING COMPLETE\\n\")\n",
    "\n",
    "        return {\"top_matches\": top_matches}\n",
    "\n",
    "# ============================================================================\n",
    "# SAMPLE USAGE BLOCK\n",
    "# (You may keep, delete, or adjust paths as needed)\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # ---------------------------------------------\n",
    "    # OPTIONAL: RUN JOB KEYWORD EXTRACTION\n",
    "    # ---------------------------------------------\n",
    "\n",
    "    try:\n",
    "        print(\"\\n\" + \"=\"*90)\n",
    "        print(\"RUNNING JOB DESCRIPTION PIPELINE DEMO\")\n",
    "        print(\"=\"*90 + \"\\n\")\n",
    "\n",
    "        job_extractor = JobKeywordExtractor()\n",
    "\n",
    "        results = job_extractor.process_jobs_csv(\n",
    "            csv_path=\"sample_jobs.csv\",      \n",
    "            job_name_col=\"job_name\",\n",
    "            job_desc_col=\"job_description\",\n",
    "            job_link_col=\"job_link\"\n",
    "        )\n",
    "\n",
    "        with open(\"job_keywords.json\", \"w\") as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "\n",
    "        print(\"\\nSaved â†’ job_keywords.json\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"\\nâš  Job extraction block skipped (no job CSV found).\")\n",
    "        print(\"  Error =\", e)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b0a03dc-cff9-4174-bd45-ecf3d6eb230d",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/marikaclark/Downloads/MockJobScrape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 13\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# -------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# 1ï¸âƒ£ CREATE SAMPLE JOB CSV (Auto-Generated)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# -------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m sample_jobs \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/marikaclark/Downloads/MockJobScrape\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     15\u001b[0m sample_jobs\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_jobs.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mðŸ“ CREATED SAMPLE FILE â†’ sample_jobs.csv\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.13/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1881\u001b[0m     f,\n\u001b[1;32m   1882\u001b[0m     mode,\n\u001b[1;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1889\u001b[0m )\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.13/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/marikaclark/Downloads/MockJobScrape'"
     ]
    }
   ],
   "source": [
    "        \n",
    "# ============================================================================\n",
    "# SAMPLE USAGE BLOCK\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    import pandas as pd\n",
    "    import json\n",
    "    sample_jobs.csv = pd.read_csv('/Users/marikaclark/Downloads/MockJobScrape.csv')\n",
    "\n",
    "    # -------------------------------------------------------------\n",
    "    # 2ï¸âƒ£ RUN JOB EXTRACTION\n",
    "    # -------------------------------------------------------------\n",
    "    try:\n",
    "        print(\"\\n\" + \"=\" * 90)\n",
    "        print(\"RUNNING JOB DESCRIPTION PIPELINE DEMO\")\n",
    "        print(\"=\" * 90 + \"\\n\")\n",
    "\n",
    "        job_extractor = JobKeywordExtractor()\n",
    "\n",
    "        results = job_extractor.process_jobs_csv(\n",
    "            csv_path=\"sample_jobs.csv\",\n",
    "            job_name_col=\"job_name\",\n",
    "            job_desc_col=\"job_description\",\n",
    "            job_link_col=\"job_link\"\n",
    "        )\n",
    "\n",
    "        # ---------------------------------------------------------\n",
    "        # 3ï¸âƒ£ SAVE OUTPUT\n",
    "        # ---------------------------------------------------------\n",
    "        with open(\"job_keywords.json\", \"w\") as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "\n",
    "        print(\"\\nðŸ’¾ Saved extracted keywords â†’ job_keywords.json\\n\")\n",
    "\n",
    "        # ---------------------------------------------------------\n",
    "        # 4ï¸âƒ£ PRINT KEYWORDS TO SCREEN\n",
    "        # ---------------------------------------------------------\n",
    "        print(\"ðŸ“Š EXTRACTED KEYWORDS\\n\")\n",
    "        for job in results[\"jobs\"]:\n",
    "            print(f\"ðŸ”¹ {job['job_name']}\")\n",
    "            print(\"   Keywords:\", \", \".join(job[\"keywords\"][:15]), \"...\\n\")\n",
    "\n",
    "        print(\"\\nðŸ”¥ MASTER KEYWORD LIST:\")\n",
    "        print(\", \".join(results[\"all_keywords\"][:40]), \"...\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"\\nâš  Job extraction block skipped.\")\n",
    "        print(\"  Error =\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f6fc309-fbe8-4af4-8549-d0266d266919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "ðŸš€ RUNNING FULL PIPELINE: COURSE â†’ RESUME â†’ JOB MATCHING\n",
      "====================================================================================================\n",
      "\n",
      "Initializing Course & Resume Keyword Extractor...\n",
      "  Loading spaCy model...\n",
      "âœ“ Extractor initialized\n",
      "\n",
      "\n",
      "ðŸ“ Reading COURSE data from:\n",
      "   /Users/marikaclark/Downloads/canvas_complete_data (13).csv\n",
      "\n",
      "================================================================================\n",
      "PROCESSING COURSES + RESUME CSV\n",
      "================================================================================\n",
      "\n",
      "Reading CSV: /Users/marikaclark/Downloads/canvas_complete_data (13).csv\n",
      "  Found 8 courses\n",
      "\n",
      "Processing course descriptions...\n",
      "  [1/8] 25F Chem1410 Section 600\n",
      "  [2/8] 25F Engineering Foundations 1_TIAN\n",
      "  [3/8] 25F Intro College Chem I Lab\n",
      "  [4/8] 25F Multivariable Calculus\n",
      "  [5/8] 2025F Econ 2010 - 091\n",
      "  [6/8] 2025F Econ 2010 Pan â€“ 102\n",
      "  [7/8] Canvas Hackathon (Professor Wright and Professor Lewis)\n",
      "  [8/8] UVA Engineering Calculus Placement (June 16th-June 20th)\n",
      "\n",
      "âœ“ Processed 8 courses\n",
      "\n",
      "Processing resume...\n",
      "  âœ“ Extracted 50 resume keywords\n",
      "\n",
      "Creating unified keyword master list...\n",
      "  âœ“ Unified list contains 100 unique keywords\n",
      "\n",
      "================================================================================\n",
      "PROCESSING COMPLETE\n",
      "================================================================================\n",
      "\n",
      "\n",
      "ðŸŽ¯ Extracted unified student keyword list!\n",
      "   â†’ 100 unique keywords\n",
      "\n",
      "\n",
      "==========================================================================================\n",
      "ðŸ”Ž RUNNING JOB SEMANTIC MATCHING ENGINE\n",
      "==========================================================================================\n",
      "\n",
      "Initializing Course & Resume Keyword Extractor...\n",
      "  Loading spaCy model...\n",
      "âœ“ Extractor initialized\n",
      "\n",
      "\n",
      "ðŸ”¹ Loading SentenceTransformer model...\n",
      "   âœ“ Embedding model loaded\n",
      "\n",
      "ðŸ“ Reading JOB listings from:\n",
      "   /Users/marikaclark/Downloads/MockJobScrape.csv\n",
      "\n",
      "==========================================================================================\n",
      "PROCESSING JOB LISTINGS + MATCHING\n",
      "==========================================================================================\n",
      "\n",
      "âš  UTF-8 failed â†’ trying latin-1\n",
      "Found 6 job listings\n",
      "\n",
      "  â–¶ [1/6] Extracting â†’ Software Engineer Intern\n",
      "  â–¶ [2/6] Extracting â†’ Summer 2026 Internship, CyberSecurity\n",
      "  â–¶ [3/6] Extracting â†’ Security Analyst Intern\n",
      "  â–¶ [4/6] Extracting â†’ Intelligence Intern - Summer 2026 (Remote)\n",
      "  â–¶ [5/6] Extracting â†’ Data & Analytics Intern\n",
      "  â–¶ [6/6] Extracting â†’ Intern - IT Security Engineer\n",
      "\n",
      "âœ“ Finished extracting job keywords\n",
      "\n",
      "Embedding USER master keyword list...\n",
      "Computing semantic similarity...\n",
      "\n",
      "ðŸ”¥ TOP 3 MATCHES:\n",
      "\n",
      "â­ Security Analyst Intern â†’ 0.773\n",
      "â­ Software Engineer Intern â†’ 0.7279\n",
      "â­ Intelligence Intern - Summer 2026 (Remote) â†’ 0.7209\n",
      "\n",
      "âœ“ SEMANTIC MATCHING COMPLETE\n",
      "\n",
      "\n",
      "ðŸ’¾ Saved â†’ job_keywords.json\n",
      "\n",
      "\n",
      "=================== DONE âœ… ===================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# MASTER EXECUTION BLOCK   (FINAL VERSION WITH REAL PATHS)\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    import pandas as pd\n",
    "    import json\n",
    "\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"ðŸš€ RUNNING FULL PIPELINE: COURSE â†’ RESUME â†’ JOB MATCHING\")\n",
    "    print(\"=\"*100 + \"\\n\")\n",
    "\n",
    "    # ============================================================\n",
    "    # 1ï¸âƒ£ PROCESS COURSE + RESUME CSV\n",
    "    # ============================================================\n",
    "    try:\n",
    "        extractor = CourseResumeKeywordExtractor()\n",
    "\n",
    "        COURSE_PATH = \"/Users/marikaclark/Downloads/canvas_complete_data (13).csv\"\n",
    "\n",
    "        print(f\"\\nðŸ“ Reading COURSE data from:\\n   {COURSE_PATH}\\n\")\n",
    "\n",
    "        course_results = extractor.process_courses_csv(\n",
    "            COURSE_PATH,\n",
    "            course_name_col=\"Course Name\",\n",
    "            course_desc_col=\"Description\",\n",
    "            resume_col=\"Resume Info\"\n",
    "        )\n",
    "\n",
    "        print(\"\\nðŸŽ¯ Extracted unified student keyword list!\")\n",
    "        print(f\"   â†’ {len(course_results['all_keywords'])} unique keywords\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"\\nâŒ ERROR WHILE PROCESSING COURSE CSV\\n\", e)\n",
    "        raise SystemExit()\n",
    "\n",
    "    # ============================================================\n",
    "    # 2ï¸âƒ£ PROCESS JOB LISTINGS + SEMANTIC MATCHING\n",
    "    # ============================================================\n",
    "    try:\n",
    "        print(\"\\n\" + \"=\"*90)\n",
    "        print(\"ðŸ”Ž RUNNING JOB SEMANTIC MATCHING ENGINE\")\n",
    "        print(\"=\"*90 + \"\\n\")\n",
    "\n",
    "        job_extractor = JobKeywordExtractor()\n",
    "\n",
    "        JOB_PATH = \"/Users/marikaclark/Downloads/MockJobScrape.csv\"\n",
    "        \n",
    "\n",
    "        print(f\"ðŸ“ Reading JOB listings from:\\n   {JOB_PATH}\\n\")\n",
    "\n",
    "        results = job_extractor.process_jobs_csv(\n",
    "            csv_path=JOB_PATH,\n",
    "            job_name_col=\"job\",\n",
    "            job_desc_col=\"job description\",\n",
    "            job_link_col=\"job link\"\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"\\nâŒ ERROR WHILE PROCESSING JOB CSV\\n\", e)\n",
    "        raise SystemExit()\n",
    "\n",
    "    # ============================================================\n",
    "    # 3ï¸âƒ£ SAVE OUTPUT (OPTIONAL)\n",
    "    # ============================================================\n",
    "    with open(\"job_keywords.json\", \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "\n",
    "    print(\"\\nðŸ’¾ Saved â†’ job_keywords.json\\n\")\n",
    "\n",
    "    print(\"\\n=================== DONE âœ… ===================\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
